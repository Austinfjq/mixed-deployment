# 调度算法模块接口
1. 增加或减少pod的接口
   
   接口功能：请求增加/减少某服务的pod，请求会被加进调度请求队列，等待下一轮调度时被读取. 
   
   URL:http://hostname:port/schedulePod
   
   支持格式:json
   
   HTTP请求方式：POST
   
   请求参数：
   
   | 参数 | 必选 | 类型 | 说明 |
   | :--- | :--- | :--- | :--- |
   | podList | true | Json数组 | 单个或多个pod |
       
   Pod：
   
   | key | 必选 | value类型 | 说明 |
   | :--- | :--- | :--- | :--- |
   | operation | true | int | 1增加，2减少 |
   | namespace | true | string | 服务所在的命名空间 |
   | serviceName | true | string | 服务名 |
   | number | true | string | 要增加/减少该类型的pod的数量 |
       
   返回字段：
   
   | 返回字段 | 字段类型 | 说明 |
   | :--- | :--- | :--- |
   | isSucceed | boolean | 是否成功加进调度请求队列 |

# 开发任务
1. 预处理策略 100%
2. 预选策略 按下面的顺序使用
    1. CheckNodeCondition 100%
    2. PodFitsResources 100%
    3. PodFitsHostPorts 100%
    4. PodMatchNodeSelector 100%
    5. PodToleratesNodeTaints 100%
    6. ~~CheckVolumeBinding 70%~~
    7. CheckNodeMemoryPressure 100%
    8. CheckNodePIDPressure 100%
    9. CheckNodeDiskPressure 100%
    10. MatchInterPodAffinity 100%
    11. PodExistingOnNode (on deleting) 100% 
3. 优选策略
    1. RequestedPriority(Least and Most) 100%
    2. BalancedResourceAllocation 100%
    3. InterPodAffinityPriority 100%
    4. SelectorSpreadPriority 100%
    5. NodeAffinityPriority 100%
    6. ~~NodePreferAvoidPodsPriority 70%~~
    7. TaintTolerationPriority 100%
    8. ~~ImageLocalityPriority 70%~~
    9. NodeLoadForecastPriority 100%
4. 挑选策略 100%

## 还没拿到的数据

### Pod的成员需要的有：

1. Labels
1. Spec.Volumes (Volume数组，仅需要Volume下的PersistentVolumeClaim下的ClaimName)
1. Spec.Containers（Container数组，仅需要Container下的Resources.Requests,Resources.Limits,image和Ports，其中Ports是ContainerPort数组，ContainerPort下仅需要hostPort, protocol, hostIP）
1. Spec.Affinity(其下递归所有成员都需要)
1. Spec.NodeSelector
1. Spec.Tolerations(Toleration数组，Toleration下仅需要key, operator, value, effect)
1. ObjectMeta.DeletionTimestamp
1. OwnerReferences(OwnerReference数组，OwnerReference下仅需要Kind和UID)

### Node的成员需要的有：

1. Labels
1. Spec.Unschedulable
1. NodeInfo下的：与memoryPressureCondition，diskPressureCondition并列出现的pidPressureCondition
1. NodeInfo下的：usedPorts(usedPorts下递归所有成员都需要)
1. NodeInfo下的：taints(Taint数组，Taint下仅需key, value, effect)
1. ~~NodeInfo下的：imageStates(ImageStateSummary数组，包括ImageStateSummary下的size和numNodes)~~
1. ~~NodeInfo下的TransientInfo下的TransNodeInfo下的 RequestedVolumes和AllocatableVolumesCount~~
1. Status.Conditions（NodeCondition数组，仅需要NodeCondition下的Type和Status）
1. kube_node_status_allocatable指标，cpu, mem, allowedPodNumber
1. ~~Annotation["scheduler.alpha.kubernetes.io/preferAvoidPods"]~~

### ~~全局feature开关值：~~

~~（例如utilfeature.DefaultFeatureGate.Enabled(features.StorageObjectInUseProtection)）~~

1. ~~StorageObjectInUseProtection~~
1. ~~BlockVolume~~
1. ~~BalanceAttachedNodeVolumes~~

### ~~所有StorageClass的列表~~

~~相当于PersistentVolumeController.classLister.List()，需要v1.StorageClass下的volumeBindingMode, provisioner, allowedTopologies~~<br />

### ~~所有的service/ReplicationController/replicaSet/statefulSet~~
~~仅需其下的Spec.Selector属性~~

### ~~所有PersistentVolumeClaim的列表~~

~~（PersistentVolumeClaim的假定绑定缓存），相当于k8s /pkg/controller/volume/persistentvolume/scheduler_binder.go的volumeBinder的pvcCache。~~

### ~~所有PersistentVolume的列表~~

~~（PersistentVolume的假定绑定缓存），相当于k8s /pkg/controller/volume/persistentvolume/scheduler_binder.go的volumeBinder的pvCache。~~

### ~~PersistentVolume的成员中需要的有：~~

1. ~~Spec.NodeAffinity~~
1. ~~Name~~
1. ~~Spec.Capacity["storage"]~~
1. ~~Spec.VolumeMode~~
1. ~~ObjectMeta.DeletionTimestamp~~
1. ~~Spec.ClaimRef是否是nil~~
1. ~~Spec.ClaimRef.Name~~
1. ~~Spec.ClaimRef.UID~~
1. ~~Spec.ClaimRef.Namespace~~
1. ~~Status.Phase~~
1. ~~Labels~~
1. ~~Annotations["volume.beta.kubernetes.io/storage-class"]~~
1. ~~Spec.StorageClassName~~
1. ~~Spec.AccessModes~~

### ~~PersistentVolumeClaim的成员中需要的有：~~

1. ~~Spec.VolumeName~~
1. ~~Name~~
1. ~~Annotations["pv.kubernetes.io/bind-completed"], Annotations["volume.kubernetes.io/selected-node"], Annotations["volume.beta.kubernetes.io/storage-class"]~~
1. ~~Spec.StorageClassName~~
1. ~~Spec.Resources.Requests["storage"]~~
1. ~~Spec.Selector下所有属性~~
1. ~~Spec.VolumeMode~~
1. ~~UID~~
1. ~~Spec.AccessModes~~

## some issues
1. 从es读取节点的预测结果

# 面向的问题
调度算法主要工作是在场景分析出的结果上再加工。确定调查的调度顺序，用最小的步骤和资源完成需要调度的pod，调度算法主要处理下面几种调度情况：
1. 添加pod：确定哪些pod需要被添加到哪些机器上；
2. 删除pod：确定哪些机器上的哪些pod需要删除；

# 流程
![image.png | left | 759x772](https://cdn.nlark.com/yuque/0/2018/png/138985/1543992803531-351d9dc4-9fa0-478b-9ae1-de334bf5eca0.png "")

# 预处理presort
做完上一轮调度后，本轮调度首先从消息队列中取出所有要增加/减少的pod。
对要增加/减少的pod排序，排序优先依据：
1. 删除 优先于 增加
2. 相对占用的资源大小，通过ServiceName查询服务画像里的[资源密集类型, 当前负载资源占用, CPU Mem request值]。如果是cpu密集型的，优先级=此pod要占用的cpu / cpu密集型pods要占用的总cpu。

# 预选predicates
按预处理产生的顺序选取要增加/减少的pod，为每一个pod评估各个节点。筛选出此pod可选的机器节点，
* 增加pod时的筛选依据(GeneralPredicates)有：
1. 检查节点是否有足够资源 PodFitsResources （需要动态数据）
2. HostPort冲突 PodFitsHostPorts （需要动态数据）
3. ~~PodSpec的NodeName字段中指定节点 PodSpec的NodeName （需要静态数据）~~
4. Pod的nodeSelector和NodeAffinity和PodAffinity定义的强约束标签 PodSelectorMatches MatchInterPodAffinity（需要静态数据）
5. PodToleratesNodeTaints （需要静态数据）
6. CheckNodeMemoryPressure, CheckNodeDiskPressure, CheckNodePIDPressure, CheckNodeCondition, （需要动态数据）
7. CheckVolumeBinding
* 减少pod时的筛选依据有：
1. 机器上有此类型的pod（需要动态数据）
* 已经明确不考虑的预选策略：
0. PodFitsHost：调度算法的调用者不会指定节点
1. NoDiskConflict：pod所需的卷是否和节点已存在的卷冲突。如果节点已经挂载了某个卷，其它同样使用这个卷的pod不能再调度到这个主机上。我们没有这个限制
2. NoVolumeZoneConflict：检查给定的zone限制前提下，检查如果在此主机上部署Pod是否存在卷冲突。我们没有设置zone-labels。
3. MaxEBSVolumeCount, MaxGCEPDVolumeCount, MaxAzureDiskVolumeCount：我们没有存储卷数量上限的设置。

# 优选priorities
* 增加pod时，对预选阶段得到的可选节点逐个打分，打分策略考虑：
1. 优先使用空闲的节点。weight1 \* 增加pod后节点空闲资源与节点总容量的比值。 LeastRequestedPriority
2. 尽量使CPU内存等利用率一致，避免出现类似CPU很高但内存很低这种情况。weight2 \* 增加pod后各个类型的资源占用最小与最大之比。BalancedResourceAllocation
3. 为了容灾考虑，尽量不把同一服务下的pod放在一起。 weight3\*其他node部署此服务的pod数与此服务总pod数之比。 SelectorSpreadPriority
4. 还可以额外定义规定某些应用倾向于放在一起、某些应用和某些node不倾向于放在一起等等软亲和约束。weight4 \* ( Σ 命中的规则对应的程度分数 )。可以包括NodeAffinity和PodAffinity中定义的软约束标签。 NodeAffinityPriority TaintTolerationPriority
5. 另外从服务负载预测可以得知接下来的一段时间哪些节点上的负载会变高或低，因此调整该node的得分。weight5 \* ( ± 即将减少/增加的资源占机器资源的比例 )

score = Σ weight i * score i 选择最大得分的机器增加这个pod，同时修改这个机器的信息上的资源信息，进行下个一个pod的处理。

* 减少pod时，对预选阶段得到的可选节点逐个打分，打分策略考虑：
1. 优先考虑繁忙的节点。weight1 \* 减少pod后节点占用资源与节点总容量的比值。
2. 尽量使CPU内存等利用率一致，避免出现类似CPU很高但内存很低这种情况。weight2 \* 减少pod后各个类型的资源占用最小与最大之比。
3. 为了容灾考虑，尽量减少同一node下的相同服务的pod。 weight3 \* 此node部署此服务的pod数与此服务总pod数之比。
4. 另外从服务负载预测可以得知接下来的一段时间哪些节点上的pod的负载会变高或低，从而影响node的负载，因此调整该node的得分。weight5 \* ( ± 即将减少/增加的资源占机器资源的比例 )

score = Σ weight i * score i 选择最大得分的机器增加这个pod，同时修改这个机器的信息上的资源信息，进行下个一个pod的处理。

# k8s v1.2 scheduler implementation
scheduleOne()
1. pod = nextPod()
2. dest = schedule(pod, nodeLister)
    1. nodes = nodeLister.List()
    2. pods = scheduler.pods.List(select all)
    3. nodeNameToInfoMap = CreateNodeNameToInfoMap(pods)
    4. filteredNodes = findNodesThatFit(pod, nodeNameToInfo, g.predicates, nodes, g.extenders)
        1. 循环对每个node，循环测试每个预选规则，如有不匹配则break，fitBool = predicate(pod, node.Name, nodeNameToInfo)
    5. priorityList = PrioritizeNodes(pod, nodeNameToInfo, g.pods, g.prioritizers, algorithm.FakeNodeLister(filteredNodes), g.extenders)
        1. 并发执行每个优选规则，一个规则循环对每个node打分
    6. return g.selectHost(priorityList)
        1. 从多个最高分node里随机选一个，随机是指round-robin.
3. ...

# k8s v1.13 scheduler implementation
scheduleOne()
1. pod = nextPod()
2. suggestedHost = schedule(pod) --> host = config.Algorithm.Schedule(pod, sched.config.NodeLister)
    1. filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes)
        1. 16线程并发地对每个node，循环测试每个预选规则，如有不匹配则break，找到足够数量的node就停止继续找，fits, failedPredicates, err := podFitsOnNode(...) --> fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse)
    2. priorityList = PrioritizeNodes(pod, nodeNameToInfo, g.pods, g.prioritizers, algorithm.FakeNodeLister(filteredNodes), g.extenders)
        1. 并发执行每个优选规则，不同规则视情况循环或者并发
    3. return g.selectHost(priorityList)
        1. 从多个最高分node里随机选一个，随机是指round-robin.
3. assumeVolumes(assumedPod, suggestedHost)
4. assume(assumedPod, suggestedHost)
5. bindVolumes(assumedPod)
6. bind(assumedPod, &v1.Binding{})
